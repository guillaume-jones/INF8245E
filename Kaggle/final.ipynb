{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import kaggle_functions as kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original images, training and test\n",
    "complete_dataset, train_dataset, valid_dataset, valid_labels = kaggle.load_train_as_dataset(return_complete_set=True)\n",
    "\n",
    "# Augment training dataset and show images to check augmentation\n",
    "train_dataset_augmented, epoch_length = kaggle.augment_dataset(train_dataset)\n",
    "kaggle.show_images(train_dataset_augmented, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and fine-tuning\n",
    "The first round of training uses augmented training data to train all architectures used in the final model.\n",
    "\n",
    "The second round of fine-tuning exposes each model to the entire dataset, non-augmented, to boost the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_paths = {\n",
    "    'model1_vgg' : (220, 20, 0.80),\n",
    "    'model2_deepervgg': (220, 25, 0.84),\n",
    "    'model3_deepervgg': (220, 25, 0.84),\n",
    "    'model4_vggres' : (180, 25, 0.80),\n",
    "    'model5_wideresnet' : (150, 15, 0.84),\n",
    "    'model6_wideresnet' : (180, 15, 0.84),\n",
    "    'model7_simplenet' : (200, 25, 0.87),\n",
    "    'model8_simplenet' : (200, 25, 0.89),\n",
    "    'model9_simplenet' : (200, 25, 0.88),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for architecture_path, (epochs, valid_patience, _) in architecture_paths.items():\n",
    "    print(f'Training {architecture_path}')\n",
    "    architecture = importlib.import_module(architecture_path)\n",
    "    # Main training\n",
    "    model, _ = kaggle.train_model(\n",
    "        architecture.Model().build(None), train_dataset_augmented, valid_dataset, \n",
    "        epochs=epochs, epoch_length=epoch_length, valid_patience=valid_patience)\n",
    "    # Fine-tuning\n",
    "    fine_model, _ = kaggle.fine_tune_model(\n",
    "        model,complete_dataset, valid_dataset, \n",
    "        epochs=4, learning_rate=1E-5)\n",
    "    # Saving for later\n",
    "    fine_model.save(f'models/{architecture_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "Gets predictions from all models and averages them for final result.\n",
    "\n",
    "\"Accuracy\" for each model was obtained by fine-tuning models on only training data, then evaluating performance on the validation set. This performance was used as \"accuracy\".\n",
    "\n",
    "This method was also used to obtain the \"power weights\" for probability and accuracy. Different values were tried for all models, and the power weights chosen had the best performance on the validation set. For this final model, the models were fine-tuning on validation data as well, so the weights had to be set in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and predicting model6/VGG_6_79\n"
     ]
    }
   ],
   "source": [
    "x_test_real = kaggle.load_test_set()\n",
    "test_predictions = np.zeros((17831, 11, len(architecture_paths)))\n",
    "\n",
    "# Loads and predicts probabilities for each image, for each model\n",
    "for index, architecture_path in enumerate(architecture_paths.keys()):\n",
    "    print(f'Loading and predicting {architecture_path}')\n",
    "    model = keras.models.load_model(f'models/{architecture_path}')\n",
    "    softmax_test_pred = np.array(tf.nn.softmax(model.predict(x_test_real)))\n",
    "    \n",
    "    test_predictions[:, :, index] = softmax_test_pred\n",
    "\n",
    "# Squares probabilities to give higher weight to more confident predictions\n",
    "modified_test_predictions = np.power(test_predictions, 2)\n",
    "# Weights each probability by the accuracy of its model, to the power 20\n",
    "for index, (_, _, accuracy) in enumerate(architecture_paths.values()):\n",
    "    modified_test_predictions[:, :, index] *= accuracy**20\n",
    "# Sums probabilities across all models to find final predictions\n",
    "modified_test_predictions = np.sum(modified_test_predictions, axis=2)\n",
    "modified_test_predictions = np.argmax(modified_test_predictions, axis=1)\n",
    "\n",
    "# Saves final predictions\n",
    "kaggle.save_test_pred('final_predictions.csv', modified_test_predictions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7beeab9d32b3e3d5812740b027dff7f9e17fcb136aeeec3859d957b5a4907666"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
