{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as reg\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "\n",
    "import kaggle_functions as kaggle\n",
    "import importlib\n",
    "importlib.reload(kaggle);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, valid_labels = kaggle.load_train_as_dataset()\n",
    "x_test_real = kaggle.load_test_set()\n",
    "\n",
    "batch_size = 128\n",
    "epoch_length = int(len(train_dataset) / batch_size)\n",
    "train_dataset_augmented = kaggle.augment_dataset(train_dataset, batch_size)\n",
    "\n",
    "model_number = 'model8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-style with residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGRes(kt.HyperModel):\n",
    "    def residual_module(self, input, filters, stride=1, bottleneck=0, l2_reg=0.00001, batch_norm=0.99):\n",
    "        # Applies bottleneck if necessary, to reduce dimensions\n",
    "        if bottleneck > 0:\n",
    "            conv_0 = layers.Conv2D(\n",
    "                bottleneck, kernel_size=(1,1),\n",
    "                padding='same', activation='relu',\n",
    "                kernel_regularizer=reg.l2(l2_reg), bias_regularizer=reg.l2(l2_reg),\n",
    "                kernel_initializer='he_normal')(input)\n",
    "        else:\n",
    "            bottleneck = filters\n",
    "            conv_0 = input\n",
    "\n",
    "        # Applies relu convolution, then linear convolution before shortcut\n",
    "        conv_1 = layers.Conv2D(\n",
    "            bottleneck, kernel_size=(3,3), strides=(stride, stride),\n",
    "            padding='same', activation='relu',\n",
    "            kernel_regularizer=reg.l2(l2_reg), bias_regularizer=reg.l2(l2_reg),\n",
    "            kernel_initializer='he_normal')(conv_0)\n",
    "        conv_2 = layers.Conv2D(\n",
    "            filters, kernel_size=(3,3), \n",
    "            padding='same', activation='linear',\n",
    "            kernel_regularizer=reg.l2(l2_reg), bias_regularizer=reg.l2(l2_reg),\n",
    "            kernel_initializer='he_normal')(conv_1)\n",
    "        \n",
    "        # Ensures shortcut is correct depth by adding a 1x1 convolution\n",
    "        if input.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(\n",
    "                filters, kernel_size=(1,1), strides=(stride,stride),\n",
    "                padding='same', activation='relu',\n",
    "                kernel_regularizer=reg.l2(l2_reg), bias_regularizer=reg.l2(l2_reg),\n",
    "                kernel_initializer='he_normal')(input)\n",
    "        else:\n",
    "            shortcut = input\n",
    "\n",
    "        # Adds shortcut\n",
    "        addition = layers.add([conv_2, shortcut])\n",
    "\n",
    "        # Batch Norm is performed in the original paper\n",
    "        addition = layers.BatchNormalization(momentum=batch_norm)(addition)\n",
    "\n",
    "        activation = layers.Activation('relu')(addition)\n",
    "        return activation\n",
    "\n",
    "    def conv_layer(self, input, filters, stride=1, kernel=3, l2_reg=0, padding='same'):\n",
    "        return layers.Conv2D(\n",
    "            filters, kernel_size=(kernel,kernel), strides=(stride,stride), \n",
    "            padding=padding, activation='relu',\n",
    "            kernel_regularizer=reg.l2(l2_reg), bias_regularizer=reg.l2(l2_reg),\n",
    "            kernel_initializer='he_uniform')(input)\n",
    "\n",
    "    def build(self, hyperparameters):\n",
    "        # Tunable hyperparameters\n",
    "        if hyperparameters is not None: \n",
    "            dense_l2_reg = hyperparameters.Float('dense_l2_reg', 0.00001, 0.001, sampling='log')\n",
    "            dense_dropout = hyperparameters.Float('dense_dropout', 0.3, 0.5, step=0.1)\n",
    "        else:\n",
    "            dense_l2_reg = 0.001\n",
    "            dense_dropout = 0.3\n",
    "\n",
    "        # Fixed hyperparameters\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        input_layer = layers.Input(shape=(96, 96, 1))\n",
    "\n",
    "        output = self.conv_layer(input_layer, 32, stride=2)\n",
    "        output = self.conv_layer(output, 32)\n",
    "        output = layers.BatchNormalization()(output)\n",
    "        \n",
    "        output = self.conv_layer(output, 64, stride=2)\n",
    "        output = self.conv_layer(output, 64)\n",
    "        output = layers.BatchNormalization()(output)\n",
    "\n",
    "        output = self.residual_module(output, 128, stride=2)\n",
    "        output = self.residual_module(output, 128)\n",
    "\n",
    "        output = self.residual_module(output, 256, stride=2, bottleneck=128)\n",
    "        output = self.residual_module(output, 256, bottleneck=128)\n",
    "\n",
    "        # Final output\n",
    "        output = layers.Flatten()(output)\n",
    "        output = layers.Dropout(dense_dropout / 2)(output)\n",
    "        output = layers.Dense(\n",
    "            128, activation='relu', kernel_initializer='he_uniform',\n",
    "            kernel_regularizer=keras.regularizers.l2(dense_l2_reg),\n",
    "            bias_regularizer=keras.regularizers.l2(dense_l2_reg))(output)\n",
    "        output = layers.Dropout(dense_dropout)(output) \n",
    "        output = layers.Dense(\n",
    "            11, kernel_regularizer=keras.regularizers.l2(dense_l2_reg),\n",
    "            bias_regularizer=keras.regularizers.l2(dense_l2_reg))(output)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "        # Create model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Nadam(learning_rate),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "# VGGRes().build(None).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 27m 14s]\n",
      "val_accuracy: 0.6181665062904358\n",
      "\n",
      "Best val_accuracy So Far: 0.6518082618713379\n",
      "Total elapsed time: 05h 02m 44s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in models/model8\\hypertuner_2021-11-28\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_l2_reg: 1e-05\n",
      "dense_dropout: 0.4000000000000001\n",
      "learning_rate: 0.001\n",
      "Score: 0.6518082618713379\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_l2_reg: 0.001\n",
      "dense_dropout: 0.4000000000000001\n",
      "learning_rate: 0.001\n",
      "Score: 0.6181665062904358\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_l2_reg: 0.01\n",
      "dense_dropout: 0.5000000000000001\n",
      "learning_rate: 0.001\n",
      "Score: 0.6009251475334167\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_l2_reg: 0.0001\n",
      "dense_dropout: 0.6000000000000001\n",
      "learning_rate: 0.001\n",
      "Score: 0.5588729977607727\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_l2_reg: 1e-05\n",
      "dense_dropout: 0.6000000000000001\n",
      "learning_rate: 0.0001\n",
      "Score: 0.5285954475402832\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_l2_reg: 1e-05\n",
      "dense_dropout: 0.2\n",
      "learning_rate: 0.0001\n",
      "Score: 0.4676198363304138\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_l2_reg: 0.001\n",
      "dense_dropout: 0.4000000000000001\n",
      "learning_rate: 0.0001\n",
      "Score: 0.4646762013435364\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_l2_reg: 0.001\n",
      "dense_dropout: 0.2\n",
      "learning_rate: 0.0001\n",
      "Score: 0.4461732506752014\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "# Took 5hrs for 8 trials with 250 epochs and LR decrease\n",
    "\n",
    "reload_tuner = False\n",
    "tuner_filepath = 'hypertuner_2021-11-28'\n",
    "\n",
    "tuner_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20, min_lr=0.00001)\n",
    "]\n",
    "\n",
    "tuner = kt.RandomSearch(VGGRes(),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=8,\n",
    "    seed=10,\n",
    "    directory=f'models/{model_number}',\n",
    "    project_name=tuner_filepath,\n",
    "    overwrite=(not reload_tuner))\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "if reload_tuner:\n",
    "    tuner.reload()\n",
    "else:\n",
    "    tuner.search(\n",
    "        train_dataset_augmented, \n",
    "        validation_data=valid_dataset.batch(128).cache(),\n",
    "        epochs=250, steps_per_epoch=epoch_length,\n",
    "        callbacks=tuner_callbacks, verbose=1)\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# model = tuner.get_best_models(2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model \n",
    "print('Building new model')\n",
    "model, history = kaggle.train_model(\n",
    "    VGGRes().build(None), train_dataset_augmented, valid_dataset, \n",
    "    epochs=200, valid_patience=30, epoch_length=epoch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = 'VGGRes_1'\n",
    "model.save(f'models/{model_number}/VGGRes_1')\n",
    "\n",
    "# Plot model statistics during training\n",
    "kaggle.plot_model_history(history, [['accuracy', 'val_accuracy'], ['loss', 'val_loss']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune model\n",
    "print('Fine-tuning model')\n",
    "fine_model, history = kaggle.fine_tune_model_filepath(\n",
    "    f'models/{model_number}/{model_name}',\n",
    "    train_dataset.batch(128).cache(), valid_dataset, \n",
    "    epochs=1, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "fine_model.save(f'models/{model_number}/VGGRes_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_evaluate = model_name # Can be changed to evaluate older models\n",
    "try:\n",
    "    loaded_model = keras.models.load_model(f'models/{model_number}/{model_to_evaluate}')\n",
    "except:\n",
    "    model = model\n",
    "\n",
    "test_pred_raw = model.predict(valid_dataset.batch(128))\n",
    "test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "kaggle.print_accuracy(valid_labels, test_pred)\n",
    "kaggle.plot_confusion_matrix(valid_labels, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test_pred = np.argmax(model.predict(x_test_real), axis=1)\n",
    "\n",
    "kaggle.save_test_pred(f'models/{model_number}/{model_name}_test_pred.csv', true_test_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7beeab9d32b3e3d5812740b027dff7f9e17fcb136aeeec3859d957b5a4907666"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
