{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import kaggle_functions as kaggle\n",
    "import model10_wideresnet\n",
    "importlib.reload(kaggle); \n",
    "importlib.reload(model10_wideresnet);\n",
    "from model10_wideresnet import Model\n",
    "\n",
    "model_number = 'model10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, valid_labels = kaggle.load_train_as_dataset()\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset_augmented, epoch_length = kaggle.augment_dataset(train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.show_images(train_dataset_augmented, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 96, 96, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_136 (Conv2D)            (None, 48, 48, 16)   160         ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 48, 48, 16)  64          ['conv2d_136[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_115 (ReLU)               (None, 48, 48, 16)   0           ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_137 (Conv2D)            (None, 24, 24, 144)  20736       ['re_lu_115[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_42 (SpatialD  (None, 24, 24, 144)  0          ['conv2d_137[0][0]']             \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 24, 24, 144)  576        ['spatial_dropout2d_42[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_116 (ReLU)               (None, 24, 24, 144)  0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_138 (Conv2D)            (None, 24, 24, 144)  186624      ['re_lu_116[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_139 (Conv2D)            (None, 24, 24, 144)  20736       ['re_lu_115[0][0]']              \n",
      "                                                                                                  \n",
      " add_54 (Add)                   (None, 24, 24, 144)  0           ['conv2d_138[0][0]',             \n",
      "                                                                  'conv2d_139[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 24, 24, 144)  576        ['add_54[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_117 (ReLU)               (None, 24, 24, 144)  0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_140 (Conv2D)            (None, 24, 24, 144)  186624      ['re_lu_117[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_43 (SpatialD  (None, 24, 24, 144)  0          ['conv2d_140[0][0]']             \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_118 (Batch  (None, 24, 24, 144)  576        ['spatial_dropout2d_43[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_118 (ReLU)               (None, 24, 24, 144)  0           ['batch_normalization_118[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_141 (Conv2D)            (None, 24, 24, 144)  186624      ['re_lu_118[0][0]']              \n",
      "                                                                                                  \n",
      " add_55 (Add)                   (None, 24, 24, 144)  0           ['conv2d_141[0][0]',             \n",
      "                                                                  'add_54[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_119 (Batch  (None, 24, 24, 144)  576        ['add_55[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_119 (ReLU)               (None, 24, 24, 144)  0           ['batch_normalization_119[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_142 (Conv2D)            (None, 12, 12, 288)  373248      ['re_lu_119[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_44 (SpatialD  (None, 12, 12, 288)  0          ['conv2d_142[0][0]']             \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 12, 12, 288)  1152       ['spatial_dropout2d_44[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_120 (ReLU)               (None, 12, 12, 288)  0           ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_143 (Conv2D)            (None, 12, 12, 288)  746496      ['re_lu_120[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_144 (Conv2D)            (None, 12, 12, 288)  373248      ['re_lu_119[0][0]']              \n",
      "                                                                                                  \n",
      " add_56 (Add)                   (None, 12, 12, 288)  0           ['conv2d_143[0][0]',             \n",
      "                                                                  'conv2d_144[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 12, 12, 288)  1152       ['add_56[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_121 (ReLU)               (None, 12, 12, 288)  0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_145 (Conv2D)            (None, 12, 12, 288)  746496      ['re_lu_121[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_45 (SpatialD  (None, 12, 12, 288)  0          ['conv2d_145[0][0]']             \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 12, 12, 288)  1152       ['spatial_dropout2d_45[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_122 (ReLU)               (None, 12, 12, 288)  0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_146 (Conv2D)            (None, 12, 12, 288)  746496      ['re_lu_122[0][0]']              \n",
      "                                                                                                  \n",
      " add_57 (Add)                   (None, 12, 12, 288)  0           ['conv2d_146[0][0]',             \n",
      "                                                                  'add_56[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_123 (Batch  (None, 12, 12, 288)  1152       ['add_57[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_123 (ReLU)               (None, 12, 12, 288)  0           ['batch_normalization_123[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_147 (Conv2D)            (None, 6, 6, 576)    1492992     ['re_lu_123[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_46 (SpatialD  (None, 6, 6, 576)   0           ['conv2d_147[0][0]']             \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_124 (Batch  (None, 6, 6, 576)   2304        ['spatial_dropout2d_46[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_124 (ReLU)               (None, 6, 6, 576)    0           ['batch_normalization_124[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_148 (Conv2D)            (None, 6, 6, 576)    2985984     ['re_lu_124[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_149 (Conv2D)            (None, 6, 6, 576)    1492992     ['re_lu_123[0][0]']              \n",
      "                                                                                                  \n",
      " add_58 (Add)                   (None, 6, 6, 576)    0           ['conv2d_148[0][0]',             \n",
      "                                                                  'conv2d_149[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 6, 6, 576)   2304        ['add_58[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_125 (ReLU)               (None, 6, 6, 576)    0           ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_150 (Conv2D)            (None, 6, 6, 576)    2985984     ['re_lu_125[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout2d_47 (SpatialD  (None, 6, 6, 576)   0           ['conv2d_150[0][0]']             \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 6, 6, 576)   2304        ['spatial_dropout2d_47[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_126 (ReLU)               (None, 6, 6, 576)    0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_151 (Conv2D)            (None, 6, 6, 576)    2985984     ['re_lu_126[0][0]']              \n",
      "                                                                                                  \n",
      " add_59 (Add)                   (None, 6, 6, 576)    0           ['conv2d_151[0][0]',             \n",
      "                                                                  'add_58[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 6, 6, 576)   2304        ['add_59[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_127 (ReLU)               (None, 6, 6, 576)    0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling2d_7 (Gl  (None, 576)         0           ['re_lu_127[0][0]']              \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 576)          0           ['global_average_pooling2d_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 11)           6347        ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,553,963\n",
      "Trainable params: 15,545,867\n",
      "Non-trainable params: 8,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model().build(None).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "298/298 [==============================] - 182s 587ms/step - loss: 9.8634 - accuracy: 0.2476 - val_loss: 8.3211 - val_accuracy: 0.0841 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "298/298 [==============================] - 171s 574ms/step - loss: 5.3821 - accuracy: 0.3139 - val_loss: 4.6055 - val_accuracy: 0.1943 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "298/298 [==============================] - 171s 574ms/step - loss: 3.5043 - accuracy: 0.3459 - val_loss: 3.1668 - val_accuracy: 0.3171 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "298/298 [==============================] - 171s 574ms/step - loss: 2.8328 - accuracy: 0.3569 - val_loss: 2.9242 - val_accuracy: 0.2876 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "298/298 [==============================] - 151s 507ms/step - loss: 2.5471 - accuracy: 0.3919 - val_loss: 2.4973 - val_accuracy: 0.3944 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.4299 - accuracy: 0.4078 - val_loss: 2.9388 - val_accuracy: 0.3267 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.3475 - accuracy: 0.4252 - val_loss: 2.4774 - val_accuracy: 0.3747 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.3033 - accuracy: 0.4395 - val_loss: 2.7588 - val_accuracy: 0.3612 - lr: 5.0000e-04\n",
      "Epoch 9/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.2529 - accuracy: 0.4476 - val_loss: 2.7682 - val_accuracy: 0.3452 - lr: 5.0000e-04\n",
      "Epoch 10/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.2092 - accuracy: 0.4558 - val_loss: 2.6528 - val_accuracy: 0.4029 - lr: 5.0000e-04\n",
      "Epoch 11/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.1850 - accuracy: 0.4698 - val_loss: 2.6356 - val_accuracy: 0.4260 - lr: 5.0000e-04\n",
      "Epoch 12/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.1522 - accuracy: 0.4808 - val_loss: 2.9164 - val_accuracy: 0.3591 - lr: 5.0000e-04\n",
      "Epoch 13/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.1330 - accuracy: 0.4970 - val_loss: 3.6283 - val_accuracy: 0.3284 - lr: 5.0000e-04\n",
      "Epoch 14/200\n",
      "298/298 [==============================] - 140s 469ms/step - loss: 2.0905 - accuracy: 0.5088 - val_loss: 2.3952 - val_accuracy: 0.4512 - lr: 5.0000e-04\n",
      "Epoch 15/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.0537 - accuracy: 0.5239 - val_loss: 2.8520 - val_accuracy: 0.4432 - lr: 5.0000e-04\n",
      "Epoch 16/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.0429 - accuracy: 0.5334 - val_loss: 2.3278 - val_accuracy: 0.4689 - lr: 5.0000e-04\n",
      "Epoch 17/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 2.0083 - accuracy: 0.5412 - val_loss: 2.4560 - val_accuracy: 0.4790 - lr: 5.0000e-04\n",
      "Epoch 18/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.9909 - accuracy: 0.5527 - val_loss: 2.1590 - val_accuracy: 0.5341 - lr: 5.0000e-04\n",
      "Epoch 19/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.9600 - accuracy: 0.5661 - val_loss: 2.5296 - val_accuracy: 0.4609 - lr: 5.0000e-04\n",
      "Epoch 20/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.9311 - accuracy: 0.5731 - val_loss: 3.1535 - val_accuracy: 0.4092 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.9163 - accuracy: 0.5890 - val_loss: 3.0155 - val_accuracy: 0.3911 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.9149 - accuracy: 0.5871 - val_loss: 2.3920 - val_accuracy: 0.4710 - lr: 5.0000e-04\n",
      "Epoch 23/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.8769 - accuracy: 0.6047 - val_loss: 2.5918 - val_accuracy: 0.3659 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.8460 - accuracy: 0.6173 - val_loss: 2.4667 - val_accuracy: 0.4647 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.8350 - accuracy: 0.6228 - val_loss: 2.5494 - val_accuracy: 0.4251 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.7995 - accuracy: 0.6396 - val_loss: 2.8677 - val_accuracy: 0.4003 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.7903 - accuracy: 0.6437 - val_loss: 2.8640 - val_accuracy: 0.4470 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "298/298 [==============================] - ETA: 0s - loss: 1.7788 - accuracy: 0.6521\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.7788 - accuracy: 0.6521 - val_loss: 2.4098 - val_accuracy: 0.5067 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.5193 - accuracy: 0.7201 - val_loss: 2.1613 - val_accuracy: 0.5307 - lr: 2.5000e-04\n",
      "Epoch 30/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.3669 - accuracy: 0.7522 - val_loss: 2.6669 - val_accuracy: 0.4706 - lr: 2.5000e-04\n",
      "Epoch 31/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.3119 - accuracy: 0.7601 - val_loss: 2.2976 - val_accuracy: 0.5151 - lr: 2.5000e-04\n",
      "Epoch 32/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.2779 - accuracy: 0.7767 - val_loss: 2.3093 - val_accuracy: 0.5505 - lr: 2.5000e-04\n",
      "Epoch 33/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.2282 - accuracy: 0.7922 - val_loss: 2.4523 - val_accuracy: 0.5244 - lr: 2.5000e-04\n",
      "Epoch 34/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.1712 - accuracy: 0.8127 - val_loss: 2.4785 - val_accuracy: 0.5328 - lr: 2.5000e-04\n",
      "Epoch 35/200\n",
      "298/298 [==============================] - 139s 467ms/step - loss: 1.1362 - accuracy: 0.8256 - val_loss: 2.7155 - val_accuracy: 0.4924 - lr: 2.5000e-04\n",
      "Epoch 36/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.1380 - accuracy: 0.8281 - val_loss: 3.1928 - val_accuracy: 0.4542 - lr: 2.5000e-04\n",
      "Epoch 37/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.0906 - accuracy: 0.8449 - val_loss: 3.0656 - val_accuracy: 0.4765 - lr: 2.5000e-04\n",
      "Epoch 38/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.0721 - accuracy: 0.8517 - val_loss: 2.7280 - val_accuracy: 0.5059 - lr: 2.5000e-04\n",
      "Epoch 39/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.0393 - accuracy: 0.8632 - val_loss: 3.2056 - val_accuracy: 0.5046 - lr: 2.5000e-04\n",
      "Epoch 40/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9932 - accuracy: 0.8786 - val_loss: 2.5003 - val_accuracy: 0.5732 - lr: 2.5000e-04\n",
      "Epoch 41/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 1.0067 - accuracy: 0.8755 - val_loss: 2.9360 - val_accuracy: 0.5294 - lr: 2.5000e-04\n",
      "Epoch 42/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9686 - accuracy: 0.8869 - val_loss: 2.5352 - val_accuracy: 0.5765 - lr: 2.5000e-04\n",
      "Epoch 43/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9464 - accuracy: 0.8960 - val_loss: 2.8922 - val_accuracy: 0.5345 - lr: 2.5000e-04\n",
      "Epoch 44/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9401 - accuracy: 0.8994 - val_loss: 3.1978 - val_accuracy: 0.4966 - lr: 2.5000e-04\n",
      "Epoch 45/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9472 - accuracy: 0.8962 - val_loss: 3.0720 - val_accuracy: 0.5282 - lr: 2.5000e-04\n",
      "Epoch 46/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9205 - accuracy: 0.9077 - val_loss: 3.0256 - val_accuracy: 0.5210 - lr: 2.5000e-04\n",
      "Epoch 47/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.8972 - accuracy: 0.9124 - val_loss: 2.7814 - val_accuracy: 0.5589 - lr: 2.5000e-04\n",
      "Epoch 48/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9068 - accuracy: 0.9117 - val_loss: 3.0364 - val_accuracy: 0.5101 - lr: 2.5000e-04\n",
      "Epoch 49/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9183 - accuracy: 0.9096 - val_loss: 3.0529 - val_accuracy: 0.5324 - lr: 2.5000e-04\n",
      "Epoch 50/200\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.8907 - accuracy: 0.9148 - val_loss: 3.2054 - val_accuracy: 0.4992 - lr: 2.5000e-04\n",
      "Epoch 51/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.9173 - accuracy: 0.9112 - val_loss: 2.9086 - val_accuracy: 0.5513 - lr: 2.5000e-04\n",
      "Epoch 52/200\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.8715 - accuracy: 0.9222\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "298/298 [==============================] - 139s 467ms/step - loss: 0.8715 - accuracy: 0.9222 - val_loss: 3.2695 - val_accuracy: 0.5231 - lr: 2.5000e-04\n",
      "Epoch 53/200\n",
      "298/298 [==============================] - 139s 467ms/step - loss: 0.7671 - accuracy: 0.9509 - val_loss: 2.5485 - val_accuracy: 0.5563 - lr: 1.2500e-04\n",
      "Epoch 54/200\n",
      "298/298 [==============================] - 139s 467ms/step - loss: 0.6808 - accuracy: 0.9642 - val_loss: 2.4065 - val_accuracy: 0.5879 - lr: 1.2500e-04\n",
      "Epoch 55/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.6505 - accuracy: 0.9680 - val_loss: 2.4606 - val_accuracy: 0.5849 - lr: 1.2500e-04\n",
      "Epoch 56/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.6149 - accuracy: 0.9720 - val_loss: 2.7617 - val_accuracy: 0.5589 - lr: 1.2500e-04\n",
      "Epoch 57/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.6158 - accuracy: 0.9698 - val_loss: 2.5123 - val_accuracy: 0.5542 - lr: 1.2500e-04\n",
      "Epoch 58/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5986 - accuracy: 0.9710 - val_loss: 2.7273 - val_accuracy: 0.5698 - lr: 1.2500e-04\n",
      "Epoch 59/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5989 - accuracy: 0.9685 - val_loss: 2.6170 - val_accuracy: 0.5900 - lr: 1.2500e-04\n",
      "Epoch 60/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5907 - accuracy: 0.9702 - val_loss: 2.9342 - val_accuracy: 0.5404 - lr: 1.2500e-04\n",
      "Epoch 61/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5896 - accuracy: 0.9681 - val_loss: 2.7555 - val_accuracy: 0.5698 - lr: 1.2500e-04\n",
      "Epoch 62/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5766 - accuracy: 0.9719 - val_loss: 2.9570 - val_accuracy: 0.5505 - lr: 1.2500e-04\n",
      "Epoch 63/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5639 - accuracy: 0.9724 - val_loss: 2.8995 - val_accuracy: 0.5416 - lr: 1.2500e-04\n",
      "Epoch 64/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5463 - accuracy: 0.9765 - val_loss: 2.7939 - val_accuracy: 0.5715 - lr: 1.2500e-04\n",
      "Epoch 65/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5528 - accuracy: 0.9730 - val_loss: 2.6239 - val_accuracy: 0.5807 - lr: 1.2500e-04\n",
      "Epoch 66/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5565 - accuracy: 0.9726 - val_loss: 3.0473 - val_accuracy: 0.5568 - lr: 1.2500e-04\n",
      "Epoch 67/200\n",
      "298/298 [==============================] - 139s 465ms/step - loss: 0.5587 - accuracy: 0.9688 - val_loss: 2.8875 - val_accuracy: 0.5744 - lr: 1.2500e-04\n",
      "Epoch 68/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5483 - accuracy: 0.9710 - val_loss: 3.3627 - val_accuracy: 0.5021 - lr: 1.2500e-04\n",
      "Epoch 69/200\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.5412 - accuracy: 0.9748\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5412 - accuracy: 0.9748 - val_loss: 3.1297 - val_accuracy: 0.5446 - lr: 1.2500e-04\n",
      "Epoch 70/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.5006 - accuracy: 0.9826 - val_loss: 2.5148 - val_accuracy: 0.5900 - lr: 6.2500e-05\n",
      "Epoch 71/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4725 - accuracy: 0.9879 - val_loss: 2.5987 - val_accuracy: 0.5786 - lr: 6.2500e-05\n",
      "Epoch 72/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4495 - accuracy: 0.9917 - val_loss: 2.5338 - val_accuracy: 0.5803 - lr: 6.2500e-05\n",
      "Epoch 73/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4447 - accuracy: 0.9883 - val_loss: 2.5581 - val_accuracy: 0.5887 - lr: 6.2500e-05\n",
      "Epoch 74/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4384 - accuracy: 0.9880 - val_loss: 2.5663 - val_accuracy: 0.5908 - lr: 6.2500e-05\n",
      "Epoch 75/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4256 - accuracy: 0.9905 - val_loss: 2.5275 - val_accuracy: 0.5862 - lr: 6.2500e-05\n",
      "Epoch 76/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4196 - accuracy: 0.9900 - val_loss: 2.7076 - val_accuracy: 0.5656 - lr: 6.2500e-05\n",
      "Epoch 77/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4141 - accuracy: 0.9902 - val_loss: 2.5725 - val_accuracy: 0.5824 - lr: 6.2500e-05\n",
      "Epoch 78/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4180 - accuracy: 0.9883 - val_loss: 2.6039 - val_accuracy: 0.5681 - lr: 6.2500e-05\n",
      "Epoch 79/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4076 - accuracy: 0.9898 - val_loss: 2.8135 - val_accuracy: 0.5643 - lr: 6.2500e-05\n",
      "Epoch 80/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4044 - accuracy: 0.9885 - val_loss: 2.7707 - val_accuracy: 0.5879 - lr: 6.2500e-05\n",
      "Epoch 81/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4044 - accuracy: 0.9893 - val_loss: 2.6136 - val_accuracy: 0.5980 - lr: 6.2500e-05\n",
      "Epoch 82/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4021 - accuracy: 0.9886 - val_loss: 2.6329 - val_accuracy: 0.5875 - lr: 6.2500e-05\n",
      "Epoch 83/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.4008 - accuracy: 0.9887 - val_loss: 2.5759 - val_accuracy: 0.5715 - lr: 6.2500e-05\n",
      "Epoch 84/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3924 - accuracy: 0.9885 - val_loss: 2.3491 - val_accuracy: 0.5917 - lr: 6.2500e-05\n",
      "Epoch 85/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3917 - accuracy: 0.9909 - val_loss: 2.4976 - val_accuracy: 0.5820 - lr: 6.2500e-05\n",
      "Epoch 86/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3847 - accuracy: 0.9893 - val_loss: 2.6628 - val_accuracy: 0.5698 - lr: 6.2500e-05\n",
      "Epoch 87/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3760 - accuracy: 0.9928 - val_loss: 2.4552 - val_accuracy: 0.5913 - lr: 6.2500e-05\n",
      "Epoch 88/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3687 - accuracy: 0.9932 - val_loss: 2.5317 - val_accuracy: 0.5883 - lr: 6.2500e-05\n",
      "Epoch 89/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3676 - accuracy: 0.9907 - val_loss: 2.4688 - val_accuracy: 0.5997 - lr: 6.2500e-05\n",
      "Epoch 90/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3748 - accuracy: 0.9874 - val_loss: 2.7328 - val_accuracy: 0.5627 - lr: 6.2500e-05\n",
      "Epoch 91/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3664 - accuracy: 0.9912 - val_loss: 2.6900 - val_accuracy: 0.5660 - lr: 6.2500e-05\n",
      "Epoch 92/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3662 - accuracy: 0.9906 - val_loss: 2.5601 - val_accuracy: 0.5845 - lr: 6.2500e-05\n",
      "Epoch 93/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3623 - accuracy: 0.9903 - val_loss: 2.5176 - val_accuracy: 0.5854 - lr: 6.2500e-05\n",
      "Epoch 94/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3549 - accuracy: 0.9905 - val_loss: 2.4074 - val_accuracy: 0.6034 - lr: 6.2500e-05\n",
      "Epoch 95/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3504 - accuracy: 0.9923 - val_loss: 2.4568 - val_accuracy: 0.5925 - lr: 6.2500e-05\n",
      "Epoch 96/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3491 - accuracy: 0.9910 - val_loss: 2.6762 - val_accuracy: 0.5812 - lr: 6.2500e-05\n",
      "Epoch 97/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3539 - accuracy: 0.9892 - val_loss: 2.7015 - val_accuracy: 0.5761 - lr: 6.2500e-05\n",
      "Epoch 98/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3531 - accuracy: 0.9891 - val_loss: 2.7361 - val_accuracy: 0.5669 - lr: 6.2500e-05\n",
      "Epoch 99/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3494 - accuracy: 0.9902 - val_loss: 2.5911 - val_accuracy: 0.5694 - lr: 6.2500e-05\n",
      "Epoch 100/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3477 - accuracy: 0.9892 - val_loss: 2.4514 - val_accuracy: 0.6056 - lr: 6.2500e-05\n",
      "Epoch 101/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3449 - accuracy: 0.9910 - val_loss: 2.5841 - val_accuracy: 0.5828 - lr: 6.2500e-05\n",
      "Epoch 102/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3403 - accuracy: 0.9906 - val_loss: 2.5026 - val_accuracy: 0.5862 - lr: 6.2500e-05\n",
      "Epoch 103/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3433 - accuracy: 0.9902 - val_loss: 2.6244 - val_accuracy: 0.5723 - lr: 6.2500e-05\n",
      "Epoch 104/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3474 - accuracy: 0.9879 - val_loss: 2.5896 - val_accuracy: 0.5828 - lr: 6.2500e-05\n",
      "Epoch 105/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3348 - accuracy: 0.9915 - val_loss: 2.5010 - val_accuracy: 0.6085 - lr: 6.2500e-05\n",
      "Epoch 106/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3376 - accuracy: 0.9904 - val_loss: 2.5352 - val_accuracy: 0.5803 - lr: 6.2500e-05\n",
      "Epoch 107/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3446 - accuracy: 0.9879 - val_loss: 2.4862 - val_accuracy: 0.5858 - lr: 6.2500e-05\n",
      "Epoch 108/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3442 - accuracy: 0.9879 - val_loss: 2.9135 - val_accuracy: 0.5530 - lr: 6.2500e-05\n",
      "Epoch 109/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3299 - accuracy: 0.9921 - val_loss: 2.5437 - val_accuracy: 0.5774 - lr: 6.2500e-05\n",
      "Epoch 110/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3298 - accuracy: 0.9904 - val_loss: 2.5608 - val_accuracy: 0.5807 - lr: 6.2500e-05\n",
      "Epoch 111/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3304 - accuracy: 0.9904 - val_loss: 2.5979 - val_accuracy: 0.5791 - lr: 6.2500e-05\n",
      "Epoch 112/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3306 - accuracy: 0.9893 - val_loss: 2.6545 - val_accuracy: 0.5719 - lr: 6.2500e-05\n",
      "Epoch 113/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3276 - accuracy: 0.9909 - val_loss: 2.6270 - val_accuracy: 0.5589 - lr: 6.2500e-05\n",
      "Epoch 114/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3266 - accuracy: 0.9890 - val_loss: 3.0473 - val_accuracy: 0.5463 - lr: 6.2500e-05\n",
      "Epoch 115/200\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.9914\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3213 - accuracy: 0.9914 - val_loss: 2.4503 - val_accuracy: 0.5854 - lr: 6.2500e-05\n",
      "Epoch 116/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.3097 - accuracy: 0.9931 - val_loss: 2.3939 - val_accuracy: 0.5799 - lr: 3.1250e-05\n",
      "Epoch 117/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2968 - accuracy: 0.9958 - val_loss: 2.4176 - val_accuracy: 0.5938 - lr: 3.1250e-05\n",
      "Epoch 118/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2898 - accuracy: 0.9972 - val_loss: 2.4814 - val_accuracy: 0.5942 - lr: 3.1250e-05\n",
      "Epoch 119/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2829 - accuracy: 0.9980 - val_loss: 2.4074 - val_accuracy: 0.5934 - lr: 3.1250e-05\n",
      "Epoch 120/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2802 - accuracy: 0.9973 - val_loss: 2.5305 - val_accuracy: 0.5971 - lr: 3.1250e-05\n",
      "Epoch 121/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2744 - accuracy: 0.9979 - val_loss: 2.4791 - val_accuracy: 0.5904 - lr: 3.1250e-05\n",
      "Epoch 122/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2726 - accuracy: 0.9975 - val_loss: 2.5346 - val_accuracy: 0.5833 - lr: 3.1250e-05\n",
      "Epoch 123/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2706 - accuracy: 0.9976 - val_loss: 2.4762 - val_accuracy: 0.5976 - lr: 3.1250e-05\n",
      "Epoch 124/200\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2720 - accuracy: 0.9964 - val_loss: 2.4217 - val_accuracy: 0.5917 - lr: 3.1250e-05\n",
      "Epoch 125/200\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9965\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "298/298 [==============================] - 139s 466ms/step - loss: 0.2677 - accuracy: 0.9965 - val_loss: 2.6022 - val_accuracy: 0.5992 - lr: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "# See {model}.py for specific training instructions, like epochs or valid_patience\n",
    "model_name = 'WideResNet_6'\n",
    "\n",
    "model, history = kaggle.train_model(\n",
    "    Model().build(None), train_dataset.batch(32), valid_dataset, \n",
    "    epochs=200, valid_patience=20, epoch_length=epoch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(f'models/{model_number}/{model_name}')\n",
    "\n",
    "# Plot model statistics during training\n",
    "kaggle.plot_model_history(history, [['accuracy', 'val_accuracy'], ['loss', 'val_loss']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Epoch 1/2\n",
      "298/298 [==============================] - 143s 458ms/step - loss: 1.7360 - accuracy: 0.9830 - val_loss: 2.3162 - val_accuracy: 0.8764\n",
      "Epoch 2/2\n",
      "298/298 [==============================] - 134s 451ms/step - loss: 1.7383 - accuracy: 0.9842 - val_loss: 2.3240 - val_accuracy: 0.8751\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune model\n",
    "fine_model, history = kaggle.fine_tune_model(\n",
    "    model,\n",
    "    train_dataset.batch(32), valid_dataset, \n",
    "    epochs=2, epoch_length=epoch_length, learning_rate=1E-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model10/WideResNet_5_84\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guillaume\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "C:\\Users\\Guillaume\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuned model\n",
    "fine_model_name = 'WideResNet_5_84'\n",
    "fine_model.save(f'models/{model_number}/{fine_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ''\n",
    "model = keras.models.load_model(f'models/{model_number}/{model_name}')\n",
    "test_pred = np.argmax(model.predict(valid_dataset.batch(128)), axis=1)\n",
    "\n",
    "kaggle.print_accuracy(valid_labels, test_pred)\n",
    "kaggle.plot_confusion_matrix(valid_labels, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'WideResNet_3'\n",
    "kaggle.generate_test_pred_filepath(f'models/{model_number}/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Stacking_0'\n",
    "kaggle.generate_test_pred(model, f'models/{model_number}/{model_name}_test_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kaggle.hypertune_model(\n",
    "    Model(), train_dataset.batch(32).cache(), valid_dataset, \n",
    "    model_number, 'hypertuner2021-12-03', trials=4, \n",
    "    epochs=11, valid_patience=3, epoch_length=epoch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project models/model10\\hypertuner2021-12-03\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from models/model10\\hypertuner2021-12-03\\tuner0.json\n",
      "Results summary\n",
      "Results in models/model10\\hypertuner2021-12-03\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_dropout: 0.5\n",
      "k: 9\n",
      "Score: 0.8048780560493469\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_dropout: 0.3\n",
      "k: 9\n",
      "Score: 0.8015138506889343\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_dropout: 0.5\n",
      "k: 6\n",
      "Score: 0.7211942672729492\n"
     ]
    }
   ],
   "source": [
    "tuner = kaggle.load_hypertuner(Model(), model_number, 'hypertuner2021-12-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models(1)[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7beeab9d32b3e3d5812740b027dff7f9e17fcb136aeeec3859d957b5a4907666"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
