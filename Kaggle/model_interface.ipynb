{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import kaggle_functions as kaggle\n",
    "import model10_wideresnet\n",
    "importlib.reload(kaggle); \n",
    "importlib.reload(model10_wideresnet);\n",
    "from model10_wideresnet import Model\n",
    "\n",
    "model_number = 'model10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, valid_labels = kaggle.load_train_as_dataset()\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset_augmented, epoch_length = kaggle.augment_dataset(train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.show_images(train_dataset_augmented, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 96, 96, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 48, 48, 16)   160         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 48, 48, 16)  64          ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 48, 48, 16)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 24, 24, 192)  27648       ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 24, 24, 192)  0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 24, 24, 192)  768        ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 24, 24, 192)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 24, 24, 192)  331776      ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 24, 24, 192)  27648       ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 24, 24, 192)  0           ['conv2d_12[0][0]',              \n",
      "                                                                  'conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 24, 24, 192)  768        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 24, 24, 192)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 12, 12, 384)  663552      ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 12, 12, 384)  0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 12, 12, 384)  1536       ['dropout_5[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 12, 12, 384)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 12, 12, 384)  1327104     ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 12, 12, 384)  663552      ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 12, 12, 384)  0           ['conv2d_15[0][0]',              \n",
      "                                                                  'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 12, 12, 384)  1536       ['add_4[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 12, 12, 384)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 6, 6, 768)    2654208     ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 6, 6, 768)    0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 6, 6, 768)   3072        ['dropout_6[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 6, 6, 768)    0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 6, 6, 768)    5308416     ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 6, 6, 768)    2654208     ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 6, 6, 768)    0           ['conv2d_18[0][0]',              \n",
      "                                                                  'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 6, 6, 768)   3072        ['add_5[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 6, 6, 768)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 768)         0           ['re_lu_13[0][0]']               \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 768)          0           ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 11)           8459        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,677,547\n",
      "Trainable params: 13,672,139\n",
      "Non-trainable params: 5,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model().build(None).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "298/298 [==============================] - 167s 516ms/step - loss: 2.2697 - accuracy: 0.2489 - val_loss: 2.6237 - val_accuracy: 0.2233 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "298/298 [==============================] - 147s 494ms/step - loss: 2.0617 - accuracy: 0.3254 - val_loss: 2.3442 - val_accuracy: 0.3003 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "298/298 [==============================] - 147s 494ms/step - loss: 1.9363 - accuracy: 0.3607 - val_loss: 2.7396 - val_accuracy: 0.2410 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "298/298 [==============================] - 142s 478ms/step - loss: 1.8536 - accuracy: 0.3931 - val_loss: 2.9307 - val_accuracy: 0.2998 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.7722 - accuracy: 0.4265 - val_loss: 2.3492 - val_accuracy: 0.3095 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.7108 - accuracy: 0.4481 - val_loss: 3.8495 - val_accuracy: 0.2065 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.6455 - accuracy: 0.4750 - val_loss: 2.9112 - val_accuracy: 0.3221 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.6077 - accuracy: 0.4938 - val_loss: 2.0321 - val_accuracy: 0.3936 - lr: 5.0000e-04\n",
      "Epoch 9/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.5647 - accuracy: 0.5034 - val_loss: 2.5349 - val_accuracy: 0.3600 - lr: 5.0000e-04\n",
      "Epoch 10/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.5294 - accuracy: 0.5211 - val_loss: 2.0698 - val_accuracy: 0.4268 - lr: 5.0000e-04\n",
      "Epoch 11/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.4950 - accuracy: 0.5293 - val_loss: 4.9477 - val_accuracy: 0.2679 - lr: 5.0000e-04\n",
      "Epoch 12/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.4640 - accuracy: 0.5422 - val_loss: 2.4263 - val_accuracy: 0.4029 - lr: 5.0000e-04\n",
      "Epoch 13/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.4404 - accuracy: 0.5531 - val_loss: 2.1317 - val_accuracy: 0.3869 - lr: 5.0000e-04\n",
      "Epoch 14/200\n",
      "298/298 [==============================] - 116s 388ms/step - loss: 1.4106 - accuracy: 0.5664 - val_loss: 1.9648 - val_accuracy: 0.4247 - lr: 5.0000e-04\n",
      "Epoch 15/200\n",
      "298/298 [==============================] - 119s 399ms/step - loss: 1.3917 - accuracy: 0.5717 - val_loss: 2.0769 - val_accuracy: 0.4605 - lr: 5.0000e-04\n",
      "Epoch 16/200\n",
      "298/298 [==============================] - 122s 410ms/step - loss: 1.3615 - accuracy: 0.5791 - val_loss: 3.0400 - val_accuracy: 0.3389 - lr: 5.0000e-04\n",
      "Epoch 17/200\n",
      "298/298 [==============================] - 122s 410ms/step - loss: 1.3386 - accuracy: 0.5910 - val_loss: 2.2752 - val_accuracy: 0.4529 - lr: 5.0000e-04\n",
      "Epoch 18/200\n",
      "298/298 [==============================] - 122s 410ms/step - loss: 1.3337 - accuracy: 0.5956 - val_loss: 3.2689 - val_accuracy: 0.3574 - lr: 5.0000e-04\n",
      "Epoch 19/200\n",
      "298/298 [==============================] - 122s 410ms/step - loss: 1.3059 - accuracy: 0.6070 - val_loss: 1.8928 - val_accuracy: 0.4647 - lr: 5.0000e-04\n",
      "Epoch 20/200\n",
      "298/298 [==============================] - 121s 405ms/step - loss: 1.2946 - accuracy: 0.6101 - val_loss: 2.2043 - val_accuracy: 0.4268 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "298/298 [==============================] - 118s 397ms/step - loss: 1.2763 - accuracy: 0.6223 - val_loss: 1.8537 - val_accuracy: 0.4882 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "298/298 [==============================] - 119s 399ms/step - loss: 1.2544 - accuracy: 0.6284 - val_loss: 1.8059 - val_accuracy: 0.4878 - lr: 5.0000e-04\n",
      "Epoch 23/200\n",
      "298/298 [==============================] - 124s 417ms/step - loss: 1.2400 - accuracy: 0.6320 - val_loss: 1.6627 - val_accuracy: 0.5421 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "298/298 [==============================] - 124s 417ms/step - loss: 1.2270 - accuracy: 0.6450 - val_loss: 2.6937 - val_accuracy: 0.4474 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "298/298 [==============================] - 125s 419ms/step - loss: 1.2221 - accuracy: 0.6384 - val_loss: 2.1441 - val_accuracy: 0.4584 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "298/298 [==============================] - 127s 425ms/step - loss: 1.2142 - accuracy: 0.6408 - val_loss: 2.1795 - val_accuracy: 0.4807 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "298/298 [==============================] - 125s 418ms/step - loss: 1.1899 - accuracy: 0.6537 - val_loss: 1.6857 - val_accuracy: 0.5383 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "298/298 [==============================] - 121s 406ms/step - loss: 1.1635 - accuracy: 0.6669 - val_loss: 2.9340 - val_accuracy: 0.4214 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "298/298 [==============================] - 125s 418ms/step - loss: 1.1729 - accuracy: 0.6630 - val_loss: 2.0743 - val_accuracy: 0.4907 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "298/298 [==============================] - 124s 415ms/step - loss: 1.1512 - accuracy: 0.6719 - val_loss: 1.9326 - val_accuracy: 0.5349 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "298/298 [==============================] - 124s 415ms/step - loss: 1.1330 - accuracy: 0.6755 - val_loss: 1.9346 - val_accuracy: 0.5395 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "298/298 [==============================] - 107s 359ms/step - loss: 1.1186 - accuracy: 0.6891 - val_loss: 2.3626 - val_accuracy: 0.4773 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "298/298 [==============================] - 124s 416ms/step - loss: 1.1145 - accuracy: 0.6845 - val_loss: 1.8703 - val_accuracy: 0.5698 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "298/298 [==============================] - 124s 415ms/step - loss: 1.1140 - accuracy: 0.6843 - val_loss: 1.5784 - val_accuracy: 0.5795 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "298/298 [==============================] - 124s 414ms/step - loss: 1.1139 - accuracy: 0.6843 - val_loss: 1.4622 - val_accuracy: 0.6127 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "298/298 [==============================] - 124s 415ms/step - loss: 1.0813 - accuracy: 0.6974 - val_loss: 2.1302 - val_accuracy: 0.5130 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "298/298 [==============================] - 123s 413ms/step - loss: 1.0794 - accuracy: 0.7028 - val_loss: 1.7165 - val_accuracy: 0.5673 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0700 - accuracy: 0.6992 - val_loss: 1.5866 - val_accuracy: 0.6030 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0590 - accuracy: 0.7079 - val_loss: 1.8306 - val_accuracy: 0.5160 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0470 - accuracy: 0.7120 - val_loss: 2.4074 - val_accuracy: 0.5093 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0459 - accuracy: 0.7162 - val_loss: 1.7317 - val_accuracy: 0.5791 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0479 - accuracy: 0.7134 - val_loss: 1.9538 - val_accuracy: 0.5500 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0306 - accuracy: 0.7241 - val_loss: 1.4804 - val_accuracy: 0.6001 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0028 - accuracy: 0.7321 - val_loss: 1.7022 - val_accuracy: 0.5866 - lr: 5.0000e-04\n",
      "Epoch 45/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0025 - accuracy: 0.7331 - val_loss: 2.1800 - val_accuracy: 0.5139 - lr: 5.0000e-04\n",
      "Epoch 46/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 1.0016 - accuracy: 0.7391 - val_loss: 1.8288 - val_accuracy: 0.5715 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.9850 - accuracy: 0.7348\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.9850 - accuracy: 0.7348 - val_loss: 2.3474 - val_accuracy: 0.5118 - lr: 5.0000e-04\n",
      "Epoch 48/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.9063 - accuracy: 0.7677 - val_loss: 1.2749 - val_accuracy: 0.6796 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.8543 - accuracy: 0.7820 - val_loss: 1.3915 - val_accuracy: 0.6657 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.8195 - accuracy: 0.7927 - val_loss: 1.2481 - val_accuracy: 0.7002 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.8137 - accuracy: 0.7969 - val_loss: 1.4120 - val_accuracy: 0.6598 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.7924 - accuracy: 0.8019 - val_loss: 1.2161 - val_accuracy: 0.7061 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.8000 - accuracy: 0.7998 - val_loss: 1.3468 - val_accuracy: 0.6745 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.7956 - accuracy: 0.8007 - val_loss: 1.4661 - val_accuracy: 0.6606 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.7752 - accuracy: 0.8066 - val_loss: 1.3463 - val_accuracy: 0.6808 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.7616 - accuracy: 0.8133 - val_loss: 1.3310 - val_accuracy: 0.6897 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.7602 - accuracy: 0.8105 - val_loss: 1.3091 - val_accuracy: 0.6859 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.7629 - accuracy: 0.8128 - val_loss: 1.2035 - val_accuracy: 0.7098 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "298/298 [==============================] - 115s 384ms/step - loss: 0.7487 - accuracy: 0.8173 - val_loss: 1.3357 - val_accuracy: 0.6939 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "298/298 [==============================] - 126s 424ms/step - loss: 0.7510 - accuracy: 0.8112 - val_loss: 1.3391 - val_accuracy: 0.6909 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "298/298 [==============================] - 129s 432ms/step - loss: 0.7297 - accuracy: 0.8188 - val_loss: 1.4118 - val_accuracy: 0.6728 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "298/298 [==============================] - 128s 431ms/step - loss: 0.7417 - accuracy: 0.8160 - val_loss: 1.2791 - val_accuracy: 0.6939 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "298/298 [==============================] - 125s 419ms/step - loss: 0.7185 - accuracy: 0.8257 - val_loss: 1.3321 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "298/298 [==============================] - 116s 388ms/step - loss: 0.7223 - accuracy: 0.8203 - val_loss: 1.1976 - val_accuracy: 0.7241 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "298/298 [==============================] - 126s 422ms/step - loss: 0.7162 - accuracy: 0.8232 - val_loss: 1.2551 - val_accuracy: 0.7077 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "298/298 [==============================] - 115s 387ms/step - loss: 0.6988 - accuracy: 0.8251 - val_loss: 1.1893 - val_accuracy: 0.7233 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "298/298 [==============================] - 90s 304ms/step - loss: 0.6982 - accuracy: 0.8294 - val_loss: 1.3606 - val_accuracy: 0.6926 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "298/298 [==============================] - 117s 393ms/step - loss: 0.7010 - accuracy: 0.8283 - val_loss: 1.3644 - val_accuracy: 0.6934 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "298/298 [==============================] - 127s 425ms/step - loss: 0.6775 - accuracy: 0.8342 - val_loss: 1.4649 - val_accuracy: 0.6724 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "  4/298 [..............................] - ETA: 2:14 - loss: 0.6384 - accuracy: 0.8828Training interrupted\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "# See {model}.py for specific training instructions, like epochs or valid_patience\n",
    "model_name = 'WideResNet_1'\n",
    "\n",
    "model, history = kaggle.train_model(\n",
    "    Model().build(None), train_dataset_augmented, valid_dataset, \n",
    "    epochs=200, valid_patience=20, epoch_length=epoch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(f'models/{model_number}/{model_name}')\n",
    "\n",
    "# Plot model statistics during training\n",
    "kaggle.plot_model_history(history, [['accuracy', 'val_accuracy'], ['loss', 'val_loss']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune model\n",
    "fine_model, history = kaggle.fine_tune_model_filepath(\n",
    "    f'models/{model_number}/WideResNet_1',\n",
    "    train_dataset.batch(32), valid_dataset, \n",
    "    epochs=3, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model7/DeeperVGG_7\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuned model\n",
    "fine_model_name = 'WideResNet_2'\n",
    "fine_model.save(f'models/{model_number}/{fine_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ''\n",
    "model = keras.models.load_model(f'models/{model_number}/{model_name}')\n",
    "test_pred = np.argmax(model.predict(valid_dataset.batch(128)), axis=1)\n",
    "\n",
    "kaggle.print_accuracy(valid_labels, test_pred)\n",
    "kaggle.plot_confusion_matrix(valid_labels, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ''\n",
    "kaggle.generate_test_pred_filepath(f'models/{model_number}/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kaggle.hypertune_model(\n",
    "    Model(), train_dataset_augmented, valid_dataset, \n",
    "    model_number, 'hypertuner2021-11-30', epochs=200, trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kaggle.load_hypertuner(Model(), model_number, '')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7beeab9d32b3e3d5812740b027dff7f9e17fcb136aeeec3859d957b5a4907666"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
