{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8245E - Assignment 3\n",
    "### Author : Guillaume Jones\n",
    "### Date : 2021-11-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conversion to bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of text\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def openTextDataset(filename):\n",
    "    # Opens a .csv text file as np array for x and y\n",
    "    with open(filename, encoding=\"utf8\") as file:\n",
    "        dataset_reader = csv.reader(file)\n",
    "        dataset = np.array(list(dataset_reader)[1:])\n",
    "        return dataset[:, 1], dataset[:, 0]\n",
    "\n",
    "def preprocessWords(text_dataset):\n",
    "    new_dataset = []\n",
    "    for text in text_dataset:\n",
    "        # Replaces ' and - with an empty character, as these are words \n",
    "        # like \"2-d\" and \"don't\" that need to be kept together\n",
    "        new_text = re.sub(r\"('|-)+\", '', text)\n",
    "\n",
    "        # Replaces other punctuation with a space, as they are characters such as , ; . () \n",
    "        # that should separate words. For example, \"ears.medication\" needs to become \"ears medication\"\n",
    "        new_text = re.sub(r\"(_|\\W)+\", ' ', new_text)\n",
    "\n",
    "        # Sends to lowercase and splits into list of words\n",
    "        new_dataset.append(new_text.lower().split(' ')[:-1])\n",
    "    return new_dataset\n",
    "\n",
    "train_x_raw, train_y = openTextDataset('medical_dataset/train.csv')\n",
    "valid_x_raw, valid_y = openTextDataset('medical_dataset/valid.csv')\n",
    "test_x_raw, test_y = openTextDataset('medical_dataset/test.csv')\n",
    "\n",
    "train_x_prep = preprocessWords(train_x_raw.tolist())\n",
    "valid_x_prep = preprocessWords(valid_x_raw.tolist())\n",
    "test_x_prep = preprocessWords(test_x_raw.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sorted list of most common words\n",
    "n_most_common = 10000\n",
    "most_common_words = dict(Counter([x for example in train_x_prep for x in example]).most_common(n_most_common))\n",
    "most_common_words_ids = dict(zip(most_common_words.keys(), range(1, n_most_common)))\n",
    "\n",
    "# Save the vocabulary in required format\n",
    "mcw_array = np.array(list(most_common_words.items()))\n",
    "mcw_array = np.insert(mcw_array, 1, np.arange(1, n_most_common + 1), axis = 1)\n",
    "np.savetxt('medical_text-vocab.txt', mcw_array, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to word ids\n",
    "def convertToIds(text_dataset, id_dict):\n",
    "    new_dataset = []\n",
    "    for example in text_dataset:\n",
    "        new_example = []\n",
    "        for word in example:\n",
    "            if word in id_dict:\n",
    "                new_example.append(id_dict[word])\n",
    "        new_dataset.append(new_example)\n",
    "    return new_dataset\n",
    "\n",
    "train_x_ids = convertToIds(train_x_prep, most_common_words_ids)\n",
    "valid_x_ids = convertToIds(valid_x_prep, most_common_words_ids)\n",
    "test_x_ids = convertToIds(test_x_prep, most_common_words_ids)\n",
    "\n",
    "# Save a dataset of ids in required format\n",
    "for (ids, y, name) in [(train_x_ids, train_y, 'train'), (valid_x_ids, valid_y, 'valid'), (test_x_ids, test_y, 'test')]:\n",
    "    id_strings = [' '.join(map(str, example)) for example in ids]\n",
    "    complete_id_dataset = np.column_stack((id_strings, y))\n",
    "    np.savetxt(f'medical_text-{name}.txt', complete_id_dataset, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary and frequency BoW generation\n",
    "\n",
    "def generateBOW(id_dataset, n_features):\n",
    "    # Create empty representation of final bag of words \n",
    "    bow_dataset = np.zeros((len(id_dataset), n_features))\n",
    "    \n",
    "    for (index, example) in enumerate(id_dataset):\n",
    "        for id in example:\n",
    "            bow_dataset[index, id - 1] += 1\n",
    "\n",
    "    bow_binary = np.where(bow_dataset > 0, 1, 0).astype(float)\n",
    "\n",
    "    # Divides each example by sum of BOW for example\n",
    "    bow_sums = bow_dataset.sum(axis=1)\n",
    "    bow_sums[np.where(bow_sums == 0)] = 1 # Prevent divisions by 0\n",
    "    bow_frequency = bow_dataset / bow_sums[:, np.newaxis]\n",
    "\n",
    "    return bow_binary, bow_frequency\n",
    "    \n",
    "train_x_binary, train_x_freq = generateBOW(train_x_ids, n_most_common)\n",
    "valid_x_binary, valid_x_freq = generateBOW(valid_x_ids, n_most_common)\n",
    "test_x_binary, test_x_freq = generateBOW(test_x_ids, n_most_common)\n",
    "complete_dataset_binary = [(train_x_binary, train_y, '\\tTrain'), (valid_x_binary, valid_y, '\\tValid'), (test_x_binary, test_y, '\\tTest')]\n",
    "complete_dataset_freq = [(train_x_freq, train_y, '\\tTrain'), (valid_x_freq, valid_y, '\\tValid'), (test_x_freq, test_y, '\\tTest')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random classifier performance:\n",
      "\tTrain - F1: 0.244\n",
      "\tValid - F1: 0.262\n",
      "\tTest - F1: 0.246\n",
      "Majority classifier performance:\n",
      "\tTrain - F1: 0.121\n",
      "\tValid - F1: 0.124\n",
      "\tTest - F1: 0.142\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def printMacroF1Score(y_pred, y_true, title):\n",
    "    f1 = f1_score(list(y_pred), list(y_true), average='macro')\n",
    "    print(title + f' - F1: {f1:.3f}')\n",
    "\n",
    "def evaluateClassifierF1(classifier, complete_dataset, classifier_name):\n",
    "    print(f'{classifier_name} classifier performance:')\n",
    "    for (x, y, name) in complete_dataset:\n",
    "        printMacroF1Score(y, classifier.predict(x), f'{name}')\n",
    "\n",
    "class RandomClassifier:\n",
    "    def __init__(self, y_train):\n",
    "        self.y_classes = list(set(y_train))\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        return np.array(random.choices(self.y_classes, k=len(x_test)))\n",
    "\n",
    "class MajorityClassifier:\n",
    "    def __init__(self, y_train):\n",
    "        self.majority_class = mode(y_train)[0][0]\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        return np.array([self.majority_class] * len(x_test))\n",
    "\n",
    "random_classifier = RandomClassifier(train_y)\n",
    "majority_classifier = MajorityClassifier(train_y)\n",
    "\n",
    "evaluateClassifierF1(random_classifier, complete_dataset_binary, 'Random')\n",
    "evaluateClassifierF1(majority_classifier, complete_dataset_binary, 'Majority')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def showGridSearchScores(grid_search, classifier_name):\n",
    "    print(f'\\n{classifier_name} grid search:')\n",
    "    for mean, params in zip(grid_search.cv_results_['mean_test_score'], \n",
    "                            grid_search.cv_results_['params']):\n",
    "        print(f'{params} : {mean:.3f}')\n",
    "\n",
    "    print(f'Best parameters : {grid_search.best_params_}')\n",
    "    return grid_search.best_params_\n",
    "\n",
    "def plotParameterScores(grid_search, parameter_to_plot, classifier_name):\n",
    "    y = []\n",
    "    x = []\n",
    "    for mean, params in zip(grid_search.cv_results_['mean_test_score'], \n",
    "                            grid_search.cv_results_['params']):\n",
    "        x.append(params[parameter_to_plot])\n",
    "        y.append(mean)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.scatter(x, y)\n",
    "    plt.ylabel('F1 score')\n",
    "    plt.xlabel(parameter_to_plot)\n",
    "    plt.title(classifier_name)\n",
    "    plt.show()\n",
    "\n",
    "# Global GridSearch parameters\n",
    "scoring = 'f1_macro'\n",
    "fold = np.repeat([-1, 0], [len(train_x_binary), len(valid_x_binary)])\n",
    "\n",
    "# Must combine train and valid sets to use PredefinedSplit\n",
    "train_valid_x_binary = np.concatenate((train_x_binary, valid_x_binary))\n",
    "train_valid_x_freq = np.concatenate((train_x_freq, valid_x_freq))\n",
    "train_valid_y = np.concatenate((train_y, valid_y))\n",
    "cv = PredefinedSplit(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes grid search\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "param_grid_bnb = [\n",
    "    {   'alpha': [0.1, 0.2, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 6, 7, 8] }\n",
    "]\n",
    "grid_search_bnb = GridSearchCV(BernoulliNB(), param_grid_bnb, scoring=scoring, cv=cv, n_jobs=4, refit=False)\n",
    "grid_search_bnb.fit(train_valid_x_binary, train_valid_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree grid search\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid_dt = [\n",
    "    {   'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [10, 20, 25, 30, 35, 40, 60, 80, 100],\n",
    "        'min_samples_leaf': [1, 2, 4, 8] },\n",
    "    {   'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [100],\n",
    "        'min_samples_leaf': [1],\n",
    "        'ccp_alpha': [0.001, 0.002, 0.003, 0.004, 0.006, 0.01] }\n",
    "]\n",
    "grid_search_bdt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, scoring=scoring, cv=cv, n_jobs=4, refit=False)\n",
    "grid_search_bdt.fit(train_valid_x_binary, train_valid_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression grid search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_lr = [\n",
    "    {   'C': [0.1, 1, 10, 100, 1000, 10000],\n",
    "        'solver' : ['newton-cg'] }\n",
    "]\n",
    "grid_search_blr = GridSearchCV(LogisticRegression(), param_grid_lr, scoring=scoring, cv=cv, n_jobs=4, refit=False)\n",
    "grid_search_blr.fit(train_valid_x_binary, train_valid_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM grid search\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "param_grid_svc = [\n",
    "    {   'C': [0.01, 0.1, 0.5, 1, 5, 10, 100],\n",
    "        'loss': ['hinge', 'squared_hinge'],\n",
    "        'max_iter': [100000] }\n",
    "]\n",
    "grid_search_bsvc = GridSearchCV(LinearSVC(), param_grid_svc, scoring=scoring, cv=cv, n_jobs=4, refit=False)\n",
    "grid_search_bsvc.fit(train_valid_x_binary, train_valid_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c. / 2.d. Hyperparameter performance on valid set and final F1-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bernoulli NB (binary) grid search:\n",
      "{'alpha': 0.1} : 0.450\n",
      "{'alpha': 0.2} : 0.451\n",
      "{'alpha': 0.5} : 0.457\n",
      "{'alpha': 1} : 0.458\n",
      "{'alpha': 1.5} : 0.460\n",
      "{'alpha': 2} : 0.466\n",
      "{'alpha': 2.5} : 0.464\n",
      "{'alpha': 3} : 0.456\n",
      "{'alpha': 4} : 0.439\n",
      "{'alpha': 5} : 0.440\n",
      "{'alpha': 6} : 0.436\n",
      "{'alpha': 7} : 0.427\n",
      "{'alpha': 8} : 0.422\n",
      "Best parameters : {'alpha': 2}\n",
      "Bernoulli NB classifier performance:\n",
      "\tTrain - F1: 0.524\n",
      "\tValid - F1: 0.466\n",
      "\tTest - F1: 0.457\n",
      "\n",
      "Decision Tree (binary) grid search:\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1} : 0.677\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2} : 0.693\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 4} : 0.694\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 8} : 0.698\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1} : 0.721\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 2} : 0.721\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 4} : 0.730\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 8} : 0.717\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 1} : 0.726\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 2} : 0.731\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 4} : 0.727\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 8} : 0.722\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1} : 0.721\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 2} : 0.737\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 4} : 0.745\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 8} : 0.734\n",
      "{'criterion': 'gini', 'max_depth': 35, 'min_samples_leaf': 1} : 0.710\n",
      "{'criterion': 'gini', 'max_depth': 35, 'min_samples_leaf': 2} : 0.713\n",
      "{'criterion': 'gini', 'max_depth': 35, 'min_samples_leaf': 4} : 0.731\n",
      "{'criterion': 'gini', 'max_depth': 35, 'min_samples_leaf': 8} : 0.726\n",
      "{'criterion': 'gini', 'max_depth': 40, 'min_samples_leaf': 1} : 0.717\n",
      "{'criterion': 'gini', 'max_depth': 40, 'min_samples_leaf': 2} : 0.724\n",
      "{'criterion': 'gini', 'max_depth': 40, 'min_samples_leaf': 4} : 0.738\n",
      "{'criterion': 'gini', 'max_depth': 40, 'min_samples_leaf': 8} : 0.722\n",
      "{'criterion': 'gini', 'max_depth': 60, 'min_samples_leaf': 1} : 0.716\n",
      "{'criterion': 'gini', 'max_depth': 60, 'min_samples_leaf': 2} : 0.725\n",
      "{'criterion': 'gini', 'max_depth': 60, 'min_samples_leaf': 4} : 0.739\n",
      "{'criterion': 'gini', 'max_depth': 60, 'min_samples_leaf': 8} : 0.724\n",
      "{'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 1} : 0.721\n",
      "{'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 2} : 0.724\n",
      "{'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 4} : 0.743\n",
      "{'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 8} : 0.726\n",
      "{'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1} : 0.706\n",
      "{'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 2} : 0.727\n",
      "{'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 4} : 0.735\n",
      "{'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 8} : 0.726\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1} : 0.681\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2} : 0.689\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 4} : 0.691\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 8} : 0.684\n",
      "{'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1} : 0.734\n",
      "{'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 2} : 0.729\n",
      "{'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 4} : 0.726\n",
      "{'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 8} : 0.707\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_leaf': 1} : 0.705\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_leaf': 2} : 0.705\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_leaf': 4} : 0.720\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_leaf': 8} : 0.700\n",
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 1} : 0.686\n",
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 2} : 0.691\n",
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 4} : 0.689\n",
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 8} : 0.694\n",
      "{'criterion': 'entropy', 'max_depth': 35, 'min_samples_leaf': 1} : 0.687\n",
      "{'criterion': 'entropy', 'max_depth': 35, 'min_samples_leaf': 2} : 0.697\n",
      "{'criterion': 'entropy', 'max_depth': 35, 'min_samples_leaf': 4} : 0.696\n",
      "{'criterion': 'entropy', 'max_depth': 35, 'min_samples_leaf': 8} : 0.694\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 1} : 0.683\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 2} : 0.691\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 4} : 0.701\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 8} : 0.693\n",
      "{'criterion': 'entropy', 'max_depth': 60, 'min_samples_leaf': 1} : 0.679\n",
      "{'criterion': 'entropy', 'max_depth': 60, 'min_samples_leaf': 2} : 0.690\n",
      "{'criterion': 'entropy', 'max_depth': 60, 'min_samples_leaf': 4} : 0.703\n",
      "{'criterion': 'entropy', 'max_depth': 60, 'min_samples_leaf': 8} : 0.692\n",
      "{'criterion': 'entropy', 'max_depth': 80, 'min_samples_leaf': 1} : 0.682\n",
      "{'criterion': 'entropy', 'max_depth': 80, 'min_samples_leaf': 2} : 0.687\n",
      "{'criterion': 'entropy', 'max_depth': 80, 'min_samples_leaf': 4} : 0.702\n",
      "{'criterion': 'entropy', 'max_depth': 80, 'min_samples_leaf': 8} : 0.698\n",
      "{'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1} : 0.672\n",
      "{'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 2} : 0.688\n",
      "{'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 4} : 0.702\n",
      "{'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 8} : 0.695\n",
      "{'ccp_alpha': 0.001, 'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1} : 0.773\n",
      "{'ccp_alpha': 0.001, 'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1} : 0.726\n",
      "{'ccp_alpha': 0.002, 'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1} : 0.778\n",
      "{'ccp_alpha': 0.002, 'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1} : 0.744\n",
      "{'ccp_alpha': 0.003, 'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1} : 0.765\n",
      "{'ccp_alpha': 0.003, 'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1} : 0.749\n",
      "{'ccp_alpha': 0.004, 'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1} : 0.717\n",
      "{'ccp_alpha': 0.004, 'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1} : 0.733\n",
      "{'ccp_alpha': 0.006, 'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1} : 0.689\n",
      "{'ccp_alpha': 0.006, 'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1} : 0.745\n",
      "{'ccp_alpha': 0.01, 'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1} : 0.660\n",
      "{'ccp_alpha': 0.01, 'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1} : 0.673\n",
      "Best parameters : {'ccp_alpha': 0.002, 'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1}\n",
      "Decision tree classifier performance:\n",
      "\tTrain - F1: 0.781\n",
      "\tValid - F1: 0.778\n",
      "\tTest - F1: 0.781\n",
      "\n",
      "Logistic Regression (binary) grid search:\n",
      "{'C': 0.1, 'solver': 'newton-cg'} : 0.680\n",
      "{'C': 1.0, 'solver': 'newton-cg'} : 0.708\n",
      "{'C': 10.0, 'solver': 'newton-cg'} : 0.734\n",
      "{'C': 100.0, 'solver': 'newton-cg'} : 0.746\n",
      "{'C': 1000.0, 'solver': 'newton-cg'} : 0.746\n",
      "{'C': 10000.0, 'solver': 'newton-cg'} : 0.740\n",
      "Best parameters : {'C': 100.0, 'solver': 'newton-cg'}\n",
      "Logistic regression classifier performance:\n",
      "\tTrain - F1: 0.908\n",
      "\tValid - F1: 0.746\n",
      "\tTest - F1: 0.773\n",
      "\n",
      "Linear SVM (binary) grid search:\n",
      "{'C': 0.01, 'loss': 'hinge', 'max_iter': 100000} : 0.702\n",
      "{'C': 0.01, 'loss': 'squared_hinge', 'max_iter': 100000} : 0.695\n",
      "{'C': 0.1, 'loss': 'hinge', 'max_iter': 100000} : 0.738\n",
      "{'C': 0.1, 'loss': 'squared_hinge', 'max_iter': 100000} : 0.742\n",
      "{'C': 0.5, 'loss': 'hinge', 'max_iter': 100000} : 0.755\n",
      "{'C': 0.5, 'loss': 'squared_hinge', 'max_iter': 100000} : 0.750\n",
      "{'C': 1.0, 'loss': 'hinge', 'max_iter': 100000} : 0.741\n",
      "{'C': 1.0, 'loss': 'squared_hinge', 'max_iter': 100000} : 0.748\n",
      "{'C': 5.0, 'loss': 'hinge', 'max_iter': 100000} : 0.732\n",
      "{'C': 5.0, 'loss': 'squared_hinge', 'max_iter': 100000} : 0.738\n",
      "{'C': 10.0, 'loss': 'hinge', 'max_iter': 100000} : 0.729\n",
      "{'C': 10.0, 'loss': 'squared_hinge', 'max_iter': 100000} : 0.733\n",
      "{'C': 100.0, 'loss': 'hinge', 'max_iter': 100000} : 0.725\n",
      "{'C': 100.0, 'loss': 'squared_hinge', 'max_iter': 100000} : 0.717\n",
      "Best parameters : {'C': 0.5, 'loss': 'hinge', 'max_iter': 100000}\n",
      "Linear SVM classifier performance:\n",
      "\tTrain - F1: 0.904\n",
      "\tValid - F1: 0.751\n",
      "\tTest - F1: 0.788\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes parameter display\n",
    "best_params_bnb = showGridSearchScores(grid_search_bnb, 'Bernoulli NB (binary)')\n",
    "\n",
    "classifier_bnb_final = BernoulliNB(**best_params_bnb).fit(train_x_binary, train_y)\n",
    "evaluateClassifierF1(classifier_bnb_final, complete_dataset_binary, 'Bernoulli NB')\n",
    "\n",
    "# Decision Tree parameter display\n",
    "best_params_bdt = showGridSearchScores(grid_search_bdt, 'Decision Tree (binary)')\n",
    "\n",
    "classifier_bdt_final = DecisionTreeClassifier(**best_params_bdt).fit(train_x_binary, train_y)\n",
    "evaluateClassifierF1(classifier_bdt_final, complete_dataset_binary, 'Decision tree')\n",
    "\n",
    "\n",
    "# Logistic regression parameter display\n",
    "best_params_blr = showGridSearchScores(grid_search_blr, 'Logistic Regression (binary)')\n",
    "\n",
    "classifier_blr_final = LogisticRegression(**best_params_blr).fit(train_x_binary, train_y)\n",
    "evaluateClassifierF1(classifier_blr_final, complete_dataset_binary, 'Logistic regression')\n",
    "\n",
    "\n",
    "# Linear SVM parameter display\n",
    "best_params_bsvc = showGridSearchScores(grid_search_bsvc, 'Linear SVM (binary)')\n",
    "\n",
    "classifier_bsvc_final = LinearSVC(**best_params_bsvc).fit(train_x_binary, train_y)\n",
    "evaluateClassifierF1(classifier_bsvc_final, complete_dataset_binary, 'Linear SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Frequency bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes grid search\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "param_grid_gnb = [\n",
    "    { 'var_smoothing': [1E-11, 1E-10, 1E-9, 1E-8] }\n",
    "]\n",
    "grid_search_gnb = GridSearchCV(GaussianNB(), param_grid_gnb, scoring=scoring, cv=cv, n_jobs=4, refit=False)\n",
    "grid_search_gnb.fit(train_valid_x_freq, train_valid_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree grid search, frequency\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid_fdt = [\n",
    "    {   'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [10, 15, 20, 25, 30, 40, 80],\n",
    "        'min_samples_leaf': [1, 2, 4, 8] },\n",
    "    {   'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [100],\n",
    "        'ccp_alpha': [0.001, 0.002, 0.003, 0.004, 0.006, 0.01] }\n",
    "]\n",
    "grid_search_fdt = GridSearchCV(DecisionTreeClassifier(), param_grid_fdt, scoring=scoring, cv=cv, n_jobs=4, refit=False)\n",
    "grid_search_fdt.fit(train_valid_x_freq, train_valid_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression grid search, frequency\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_flr = [\n",
    "    {   'C': [1, 10, 100, 1000, 1E4, 1E5, 1E6, 1E7, 1E8],\n",
    "        'max_iter': [1000],\n",
    "        'solver' : ['liblinear'] }\n",
    "]\n",
    "grid_search_flr = GridSearchCV(LogisticRegression(), param_grid_flr, scoring=scoring, cv=cv, n_jobs=4, refit=False)\n",
    "grid_search_flr.fit(train_valid_x_freq, train_valid_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM grid search, frequency\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "param_grid_fsvc = [\n",
    "    {   'C': [1, 10, 100, 1000, 1E4, 1E5, 1E6],\n",
    "        'loss': ['hinge', 'squared_hinge'],\n",
    "        'max_iter': [100000],\n",
    "        'tol': [0.01] }\n",
    "]\n",
    "grid_search_fsvc = GridSearchCV(LinearSVC(), param_grid_fsvc, scoring=scoring, cv=cv, n_jobs=4, refit=False)\n",
    "grid_search_fsvc.fit(train_valid_x_freq, train_valid_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b. / 3.c. Hyperparameter performance on valid set and final F1-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian NB (frequency) grid search:\n",
      "{'var_smoothing': 1e-11} : 0.361\n",
      "{'var_smoothing': 1e-10} : 0.372\n",
      "{'var_smoothing': 1e-09} : 0.368\n",
      "{'var_smoothing': 1e-08} : 0.365\n",
      "Best parameters : {'var_smoothing': 1e-10}\n",
      "GNB classifier performance:\n",
      "\tTrain - F1: 0.679\n",
      "\tValid - F1: 0.372\n",
      "\tTest - F1: 0.352\n",
      "\n",
      "Decision tree (frequency) grid search:\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1} : 0.702\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2} : 0.704\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 4} : 0.702\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 8} : 0.709\n",
      "{'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1} : 0.717\n",
      "{'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 2} : 0.723\n",
      "{'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 4} : 0.721\n",
      "{'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 8} : 0.724\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1} : 0.759\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 2} : 0.761\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 4} : 0.756\n",
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 8} : 0.737\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 1} : 0.712\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 2} : 0.722\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 4} : 0.722\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 8} : 0.701\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1} : 0.725\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 2} : 0.719\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 4} : 0.717\n",
      "{'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 8} : 0.707\n",
      "{'criterion': 'gini', 'max_depth': 40, 'min_samples_leaf': 1} : 0.698\n",
      "{'criterion': 'gini', 'max_depth': 40, 'min_samples_leaf': 2} : 0.704\n",
      "{'criterion': 'gini', 'max_depth': 40, 'min_samples_leaf': 4} : 0.701\n",
      "{'criterion': 'gini', 'max_depth': 40, 'min_samples_leaf': 8} : 0.712\n",
      "{'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 1} : 0.692\n",
      "{'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 2} : 0.704\n",
      "{'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 4} : 0.699\n",
      "{'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 8} : 0.705\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1} : 0.699\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2} : 0.698\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 4} : 0.698\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 8} : 0.705\n",
      "{'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1} : 0.747\n",
      "{'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 2} : 0.751\n",
      "{'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 4} : 0.751\n",
      "{'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 8} : 0.741\n",
      "{'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1} : 0.714\n",
      "{'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 2} : 0.711\n",
      "{'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 4} : 0.704\n",
      "{'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 8} : 0.701\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_leaf': 1} : 0.689\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_leaf': 2} : 0.684\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_leaf': 4} : 0.700\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_leaf': 8} : 0.699\n",
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 1} : 0.682\n",
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 2} : 0.695\n",
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 4} : 0.694\n",
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 8} : 0.692\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 1} : 0.681\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 2} : 0.690\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 4} : 0.686\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 8} : 0.694\n",
      "{'criterion': 'entropy', 'max_depth': 80, 'min_samples_leaf': 1} : 0.679\n",
      "{'criterion': 'entropy', 'max_depth': 80, 'min_samples_leaf': 2} : 0.679\n",
      "{'criterion': 'entropy', 'max_depth': 80, 'min_samples_leaf': 4} : 0.696\n",
      "{'criterion': 'entropy', 'max_depth': 80, 'min_samples_leaf': 8} : 0.699\n",
      "{'ccp_alpha': 0.001, 'criterion': 'gini', 'max_depth': 100} : 0.751\n",
      "{'ccp_alpha': 0.001, 'criterion': 'entropy', 'max_depth': 100} : 0.724\n",
      "{'ccp_alpha': 0.002, 'criterion': 'gini', 'max_depth': 100} : 0.769\n",
      "{'ccp_alpha': 0.002, 'criterion': 'entropy', 'max_depth': 100} : 0.740\n",
      "{'ccp_alpha': 0.003, 'criterion': 'gini', 'max_depth': 100} : 0.712\n",
      "{'ccp_alpha': 0.003, 'criterion': 'entropy', 'max_depth': 100} : 0.759\n",
      "{'ccp_alpha': 0.004, 'criterion': 'gini', 'max_depth': 100} : 0.714\n",
      "{'ccp_alpha': 0.004, 'criterion': 'entropy', 'max_depth': 100} : 0.748\n",
      "{'ccp_alpha': 0.006, 'criterion': 'gini', 'max_depth': 100} : 0.689\n",
      "{'ccp_alpha': 0.006, 'criterion': 'entropy', 'max_depth': 100} : 0.744\n",
      "{'ccp_alpha': 0.01, 'criterion': 'gini', 'max_depth': 100} : 0.644\n",
      "{'ccp_alpha': 0.01, 'criterion': 'entropy', 'max_depth': 100} : 0.697\n",
      "Best parameters : {'ccp_alpha': 0.002, 'criterion': 'gini', 'max_depth': 100}\n",
      "Decision tree classifier performance:\n",
      "\tTrain - F1: 0.780\n",
      "\tValid - F1: 0.768\n",
      "\tTest - F1: 0.757\n",
      "\n",
      "Logistic regression (frequency) grid search:\n",
      "{'C': 1, 'max_iter': 1000, 'solver': 'liblinear'} : 0.320\n",
      "{'C': 10, 'max_iter': 1000, 'solver': 'liblinear'} : 0.388\n",
      "{'C': 100, 'max_iter': 1000, 'solver': 'liblinear'} : 0.471\n",
      "{'C': 1000, 'max_iter': 1000, 'solver': 'liblinear'} : 0.505\n",
      "{'C': 10000.0, 'max_iter': 1000, 'solver': 'liblinear'} : 0.565\n",
      "{'C': 100000.0, 'max_iter': 1000, 'solver': 'liblinear'} : 0.640\n",
      "{'C': 1000000.0, 'max_iter': 1000, 'solver': 'liblinear'} : 0.659\n",
      "{'C': 10000000.0, 'max_iter': 1000, 'solver': 'liblinear'} : 0.677\n",
      "{'C': 100000000.0, 'max_iter': 1000, 'solver': 'liblinear'} : 0.664\n",
      "Best parameters : {'C': 10000000.0, 'max_iter': 1000, 'solver': 'liblinear'}\n",
      "Logistic regression classifier performance:\n",
      "\tTrain - F1: 0.909\n",
      "\tValid - F1: 0.663\n",
      "\tTest - F1: 0.677\n",
      "\n",
      "Linear SVM (frequency) grid search:\n",
      "{'C': 1, 'loss': 'hinge', 'max_iter': 100000, 'tol': 0.01} : 0.408\n",
      "{'C': 1, 'loss': 'squared_hinge', 'max_iter': 100000, 'tol': 0.01} : 0.392\n",
      "{'C': 10, 'loss': 'hinge', 'max_iter': 100000, 'tol': 0.01} : 0.456\n",
      "{'C': 10, 'loss': 'squared_hinge', 'max_iter': 100000, 'tol': 0.01} : 0.470\n",
      "{'C': 100, 'loss': 'hinge', 'max_iter': 100000, 'tol': 0.01} : 0.496\n",
      "{'C': 100, 'loss': 'squared_hinge', 'max_iter': 100000, 'tol': 0.01} : 0.492\n",
      "{'C': 1000, 'loss': 'hinge', 'max_iter': 100000, 'tol': 0.01} : 0.555\n",
      "{'C': 1000, 'loss': 'squared_hinge', 'max_iter': 100000, 'tol': 0.01} : 0.589\n",
      "{'C': 10000.0, 'loss': 'hinge', 'max_iter': 100000, 'tol': 0.01} : 0.688\n",
      "{'C': 10000.0, 'loss': 'squared_hinge', 'max_iter': 100000, 'tol': 0.01} : 0.682\n",
      "{'C': 100000.0, 'loss': 'hinge', 'max_iter': 100000, 'tol': 0.01} : 0.700\n",
      "{'C': 100000.0, 'loss': 'squared_hinge', 'max_iter': 100000, 'tol': 0.01} : 0.701\n",
      "{'C': 1000000.0, 'loss': 'hinge', 'max_iter': 100000, 'tol': 0.01} : 0.668\n",
      "{'C': 1000000.0, 'loss': 'squared_hinge', 'max_iter': 100000, 'tol': 0.01} : 0.693\n",
      "Best parameters : {'C': 100000.0, 'loss': 'squared_hinge', 'max_iter': 100000, 'tol': 0.01}\n",
      "Linear SVM classifier performance:\n",
      "\tTrain - F1: 0.909\n",
      "\tValid - F1: 0.694\n",
      "\tTest - F1: 0.724\n"
     ]
    }
   ],
   "source": [
    "# # Gaussian Naive Bayes parameter display\n",
    "best_params_gnb = showGridSearchScores(grid_search_gnb, 'Gaussian NB (frequency)')\n",
    "\n",
    "classifier_gnb_final = GaussianNB(**best_params_gnb).fit(train_x_freq, train_y)\n",
    "evaluateClassifierF1(classifier_gnb_final, complete_dataset_freq, 'GNB')\n",
    "\n",
    "# Decision Tree (frequency) parameter display\n",
    "best_params_fdt = showGridSearchScores(grid_search_fdt, 'Decision tree (frequency)')\n",
    "\n",
    "classifier_fdt_final = DecisionTreeClassifier(**best_params_fdt).fit(train_x_freq, train_y)\n",
    "evaluateClassifierF1(classifier_fdt_final, complete_dataset_freq, 'Decision tree')\n",
    "\n",
    "# Logistic regression (frequency) parameter display\n",
    "best_params_flr = showGridSearchScores(grid_search_flr, 'Logistic regression (frequency)')\n",
    "\n",
    "classifier_flr_final = LogisticRegression(**best_params_flr).fit(train_x_freq, train_y)\n",
    "evaluateClassifierF1(classifier_flr_final, complete_dataset_freq, 'Logistic regression')\n",
    "\n",
    "# Linear SVM (frequency) parameter display\n",
    "best_params_fsvc = showGridSearchScores(grid_search_fsvc, 'Linear SVM (frequency)')\n",
    "\n",
    "classifier_fsvc_final = LinearSVC(**best_params_fsvc).fit(train_x_freq, train_y)\n",
    "evaluateClassifierF1(classifier_fsvc_final, complete_dataset_freq, 'Linear SVM')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7beeab9d32b3e3d5812740b027dff7f9e17fcb136aeeec3859d957b5a4907666"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
